{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task VIII: Vision Transformer ####\n",
    "\n",
    "This task implements a simple Vision Transformer (ViT) architecture to classify 16x16 MNIST images into their respective numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction ##\n",
    "The data is extracted in much of the same way as for the diffusion model, but instead of having one dataset and data_loader, I split them into train and test datasets with their respective loaders. This is so that the model can be tested on previously unseen images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model ## \n",
    "The model architecture largely follows this website: https://itp.uni-frankfurt.de/~gros/StudentProjects/WS22_23_VisualTransformer/\n",
    "\n",
    "They use Tensorflow, but I decided to go with PyTorch to be consistent. I omitted some of the steps such as data augmentation (transformations in PyTorch) due to the fact that the MNIST dataset is already quite large, and I wanted the training to be faster even if it meant less accuracy. I also simplified the architecture quite a bit because I found that a more close to the website implementation was taking awhile. \n",
    "\n",
    "In the end, I only had a VisionTransformer block with PatchEmbedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size=16, patch_size=4, in_channels=1, embed_dim=96):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=16, patch_size=4, num_classes=10, embed_dim=96, num_heads=4, mlp_dim=2048, num_layers=16):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size, patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ## \n",
    "I scaled down the model to 10 epochs (the original code used 100) in the training loop. The testing just outputs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\realc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result ##\n",
    "The accuracy is 11.35%, which is barely above random. I think this is due to the fact that I both scaled down the model and the number of epochs. My loss stayed at around 2.3 for the whole process, indicating that perhaps the model architecture could be made more efficient. I didn't have too much time to spend on researching ViT techniques, but I do think it's an interesting topic to look into with QML. To make this model quantum, I'd probably consider a quantum transformer to implement into the visual transformer architecture. Unfortunately, I didn't get to play around with the self-attention mechanism in this model, but this is potentially where the quantum advantage could come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Loss: 2.3173\n",
      "Epoch [2/10]\n",
      "Loss: 2.3057\n",
      "Epoch [3/10]\n",
      "Loss: 2.3036\n",
      "Epoch [4/10]\n",
      "Loss: 2.3028\n",
      "Epoch [5/10]\n",
      "Loss: 2.3026\n",
      "Epoch [6/10]\n",
      "Loss: 2.3023\n",
      "Epoch [7/10]\n",
      "Loss: 2.3022\n",
      "Epoch [8/10]\n",
      "Loss: 2.3019\n",
      "Epoch [9/10]\n",
      "Loss: 2.3019\n",
      "Epoch [10/10]\n",
      "Loss: 2.3018\n",
      "Test Accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "# Train and test the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "test(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
